{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Required Packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import dateparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the site using Selenium webdrivers in Firefox browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: We need to specify the path of geckodriver.exe which can be installed seperately to run the drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToDriver='E:\\Learning\\Learning\\Techolution\\geckodriver.exe'\n",
    "browser = webdriver.Firefox(executable_path=pathToDriver) #replace with .Firefox(), or with the browser of your choice\n",
    "browser.get('https://techolution.app.param.ai/jobs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using inspect element from the website I've managed to intercept that the data is getting stored in the class ui bottom aligned grid. All i need is to find all the occurances and then using list comprehension save it in list and later in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav1 = browser.find_elements_by_xpath(\"//div[@class='ui bottom aligned grid']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [x.text for x in nav1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame({'rawdata':titles})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parsing of the dates which are there in the format '1 day ago', '2 days ago', '1 month ago' is done using the python package dateparser and then sorted according to the dates obtained to show Most old first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data=pd.DataFrame()\n",
    "final_data['Role']=data['rawdata'].apply(lambda x: x.split('\\n')[0])\n",
    "final_data['Description']=data['rawdata'].apply(lambda x: x.split('\\n')[1])\n",
    "final_data['JobPosted']=data['rawdata'].apply(lambda x: x.split('\\n')[2])\n",
    "final_data['ParsedDates']=final_data['JobPosted'].apply(lambda x: dateparser.parse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data=final_data[['Role','Description','JobPosted','ParsedDates']].sort_values('ParsedDates').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please specify the path to store the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[['Role','Description','JobPosted']].to_csv(\"E:\\Learning\\Learning\\Techolution\\JobOpenings.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
